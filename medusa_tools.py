from datetime import datetime, timedelta
from typing import Union, Tuple, Optional, KeysView
from edge_extractor import Edge
import bw2data as bd
import bw2calc as bc
import bw_processing as bwp
import uuid
import logging
import numpy as np
import warnings
import pandas as pd

# for prepare_medusa_lca_inputs
from bw2data import (
    Database,
    Method,
    Normalization,
    Weighting,
    databases,
    methods,
    normalizations,
    projects,
    weightings,
)
from bw2data.backends.schema import ActivityDataset as AD
from bw2data.backends.schema import get_id
from bw2data.errors import Brightway2Project

from utils import extract_date_as_integer, check_database_names

def create_demand_timing_dict(timeline: pd.DataFrame, demand: dict) -> dict:
    """
    Generate a dictionary mapping producer (key) to reference timing (currently YYYYMM) (value) for specific demands. 
    It searches the timeline for those rows that contain the functional units (demand-processes as producer and -1 as consumer) and returns the time of the demand.
    
    :param timeline: Timeline DataFrame, generated by `create_grouped_edge_dataframe`.
    :param demand: Demand dict
    
    :return: Dictionary mapping producer ids to reference timing (currently YYYYMM) for the specified demands.
    """
    demand_ids = [bd.get_activity(key).id for key in demand.keys()]
    demand_rows = timeline[timeline['producer'].isin(demand_ids) & (timeline['consumer'] == -1)]
    return {row.producer: extract_date_as_integer(row.date) for row in demand_rows.itertuples()}  

def create_grouped_edge_dataframe(tl: list, database_date_dict: dict, temporal_grouping: str = 'year', interpolation_type: str ="linear") -> pd.DataFrame:
    """
    Create a grouped edge dataframe. 
    
    Edges that occur at different times within the same unit of time are grouped together. Th etemporal grouping is currently possible by year and month. Hopefully soon also by day and hour.

    The column "interpolation weights" assigns the ratio [0-1] of the edge's amount to be taken from the database with the closest time of representativeness. 
    Available interpolation types are:
        - "linear": linear interpolation between the two closest databases, based on temporal distance
        - "closest": closest database is assigned 1

    :param tl: Timeline containing edge information.
    :param database_date_dict: Mapping dictionary between the time of representativeness (key) and name of database (value)
    :param temporal_grouping: Type of temporal grouping (default is "year"). Available options are "year", "month", "day" and "hour" (TODO fix day and hour)
    :param interpolation_type: Type of interpolation (default is "linear").
    :return: Grouped edge dataframe.
    """
       
    def extract_edge_data(edge: Edge) -> dict:
        """
        Stores the attributes of an Edge instance in a dictionary.

        :param edge: Edge instance
        :return: Dictionary with attributes of the edge 
        """
        return {
            "datetime": edge.distribution.date,
            "amount": edge.distribution.amount,
            "producer": edge.producer,
            "consumer": edge.consumer,
            "leaf": edge.leaf
        }
    
    def get_consumer_name(id: int) -> str:
        """
        Returns the name of consumer node. 
        If consuming node is the functional unit, returns -1.

        :param id: Id of node
        :return: string of node's name or -1
        """
        try:
            return bd.get_node(id=id)['name']
        except:
            return '-1' #functional unit
        
    # check if database names match with databases in BW project
    check_database_names(database_date_dict)
        
    # Check if temporal_grouping is a valid value
    valid_temporal_groupings = ['year', 'month', 'day', 'hour']
    
    if temporal_grouping not in valid_temporal_groupings:
        raise ValueError(f"Invalid value for 'temporal_grouping'. Allowed values are {valid_temporal_groupings}.")
    
    # warning about day and hour not working yet
    if temporal_grouping in ['day', 'hour']:
        raise ValueError(f"Sorry, but temporal grouping is not yet available for 'day' and 'hour'.")
    
    # Extract edge data into a list of dictionaries
    edges_data = [extract_edge_data(edge) for edge in tl]
    
    # Convert list of dictionaries to dataframe
    edges_df = pd.DataFrame(edges_data)
    
    # Explode datetime and amount columns
    edges_df = edges_df.explode(['datetime', 'amount'])
    
    # Extract different temporal groupings from datetime column: year to hour
    edges_df['year'] = edges_df['datetime'].apply(lambda x: x.year)
    edges_df['year_month'] = edges_df['datetime'].apply(lambda x: x.strftime("%Y-%m"))
    edges_df['year_month_day'] = edges_df['datetime'].apply(lambda x: x.strftime("%Y-%m-%d"))
    edges_df['year_month_day_hour'] = edges_df['datetime'].apply(lambda x: x.strftime("%Y-%m-%dT%H"))
          
    # Group by selected temporal scope & convert temporal grouping to datetime format: 
    # #FIXME: each assignment uses the first timestamp in the respective period, 
    # e.g. for year: 2024-12-31 gets turned into 2024, possibly grouped with other 2024 rows and then reassigned to 2024-01-01
    if temporal_grouping == 'year': 
        grouped_edges = edges_df.groupby(['year', 'producer', 'consumer'])['amount'].sum().reset_index()
        grouped_edges['date'] = grouped_edges['year'].apply(lambda x: datetime(x, 1, 1))
    elif temporal_grouping == 'month':
        grouped_edges = edges_df.groupby(['year', 'year_month', 'producer', 'consumer'])['amount'].sum().reset_index() 
        grouped_edges['date'] = grouped_edges['year_month'].apply(lambda x: datetime.strptime(x, '%Y-%m'))
    elif temporal_grouping == 'day': 
        grouped_edges = edges_df.groupby(['year', 'year_month_day', 'producer', 'consumer'])['amount'].sum().reset_index()
        grouped_edges['date'] = grouped_edges['year_month_day'].apply(lambda x: datetime.strptime(x, "%Y-%m-%d"))
    elif temporal_grouping == 'hour': 
        grouped_edges = edges_df.groupby(['year', 'year_month_day_hour', 'producer', 'consumer'])['amount'].sum().reset_index()
        grouped_edges['date'] = grouped_edges['year_month_day_hour'].apply(lambda x: datetime.strptime(x, "%Y-%m-%dT%H"))
    else:
        raise ValueError(f"Sorry, but {temporal_grouping} temporal scope grouping is not available yet.")
    
    
    # Add interpolation weights to the dataframe
    grouped_edges = add_column_interpolation_weights_to_timeline(grouped_edges, database_date_dict, interpolation_type=interpolation_type)
    
    # Retrieve producer and consumer names
    grouped_edges['producer_name'] = grouped_edges.producer.apply(lambda x: bd.get_node(id=x)["name"])
    grouped_edges['consumer_name'] = grouped_edges.consumer.apply(get_consumer_name)
    
    grouped_edges['timestamp'] = grouped_edges['year']  # for now just year but could be calling the function --> extract_date_as_integer(grouped_edges['date'])
    # grouped_edges = grouped_edges[['date', 'year', 'producer', 'producer_name', 'consumer', 'consumer_name', 'amount', 'interpolation_weights']]
     #TODO: remove year, since we now have flexible time grouping and everything is stored in date. Currently still kept for clarity
    return grouped_edges

def create_datapackage_from_edge_timeline(
    timeline: pd.DataFrame, 
    database_date_dict: dict, 
    demand_timing: dict,
    datapackage: Optional[bwp.Datapackage] = None,
    name: Optional[str] = None,
) -> bwp.Datapackage:
    """
    Creates patches from a given timeline. Patches are datapackages that add or overwrite datapoints in the LCA matrices before LCA calculations.
    
    The heavy lifting of this function happens in its inner function "add_row_to_datapackage":
    Here, each node with a temporal distribution is "exploded", which means each occurrence of this node (e.g. steel production on 2020-01-01 
    and steel production 2015-01-01) becomes separate new node with its own unique id. The exchanges on these node-clones get relinked to the activities 
    with the same name, reference product and location as previously, but now from the corresponding databases in time.

    The new node-clones also need a 1 on the diagonal of their technosphere matrix, which symbolizes the production of the new clone-reference product.

    Inputs:
    timeline: list
        A timeline of edges, typically created from EdgeExtracter.create_edge_timeline()
    database_date_dict: dict
        A dict of the available prospective database: their temporal representativeness (key) and their names (value).
    demand_timing: dict
        A dict of the demand ids and the timing they should be linked to. Can be created using create_demand_timing_dict().
    datapackage: Optional[bwp.Datapackage]
        Append to this datapackage, if available. Otherwise create a new datapackage.
    name: Optional[str]
        Name of this datapackage resource.

    Returns:
    bwp.Datapackage
        A list of patches formatted as datapackages.
    """

    def add_row_to_datapackage(row: pd.core.frame, datapackage: bwp.Datapackage,
                               database_date_dict: dict, demand_timing: dict,
                               new_nodes: set, consumer_timestamps: dict) -> None: 
        """
        create a datapackage for the new edges based on a row from the timeline DataFrame.

        :param row: A row from the timeline DataFrame.
        :param database_dates_dict: Dictionary of available prospective database dates and their names.
        :param demand_timing: Dictionary of the demand ids and the dates they should be linked to. Can be created using create_demand_timing_dict().
        :param new_nodes: empty set to which new node ids are added
        :return: None, but adds new edges to the set new_nodes and adds a patch for this new edge to the bwp.Datapackage
    """
        # print('Current row:', row.year, ' | ', row.producer_name, ' | ', row.consumer_name)

        if row.consumer == -1: # ? Why? Might be in the timeline-building code that starts graph traversal at FU and directly goes down the supply chain
            # print('Row contains the functional unit - exploding to new time-specific node')
            new_producer_id = row.producer*1000000+row.timestamp
            new_nodes.add(new_producer_id)
            # print(f'New producer id = {new_producer_id}')
            # print()
            return
        
        new_consumer_id = row.consumer*1000000+consumer_timestamps[row.consumer]
        # print(f'New consumer id = {new_consumer_id}')
        # print(f'New added year= {extract_date_as_integer(row.date)}')
        new_producer_id = row.producer*1000000+row.timestamp # In case the producer comes from a background database, we overwrite this. It currently still gets added to new_nodes, but this is not necessary.
        new_nodes.add(new_consumer_id)
        new_nodes.add(new_producer_id) 
        previous_producer_id = row.producer
        previous_producer_node = bd.get_node(id=previous_producer_id) # in future versions, insead of getting node, just provide list of producer ids
        
        # Check if previous producer comes from foreground database
        if not previous_producer_node['database'] in database_date_dict.values():
            
            # create new consumer id if consumer is the functional unit
            if row.consumer in demand_timing.keys():
                new_consumer_id = row.consumer*1000000+consumer_timestamps[row.consumer] #Why?

            # print('Row contains internal foreground edge - exploding to new time-specific nodes')
            # print(f'New producer id = {new_producer_id}')
            # print(f'New consumer id = {new_consumer_id}')
            # print()
            datapackage.add_persistent_vector(
                        matrix="technosphere_matrix",
                        name=uuid.uuid4().hex,
                        data_array=np.array([row.amount], dtype=float),
                        indices_array=np.array(
                            [(new_producer_id, new_consumer_id)], #FIXME: I think if orevious producer comes from foreground database, new_producer_id should be assigned back to original producer_id from foreground database.
                            dtype=bwp.INDICES_DTYPE,
                        ),
                        flip_array=np.array([True], dtype=bool),
                )
        
        else:   # Previous producer comes from background database
            # print('Row links to background database')

            # create new consumer id if consumer is the functional unit
            if row.consumer in demand_timing.keys():
                new_consumer_id = row.consumer*1000000+consumer_timestamps[row.consumer] #reduced by two digits due to OverflowError: Python int too large to convert to C long

            # Create new edges based on interpolation_weights from the row
            for database, share in row.interpolation_weights.items():
                # print(f'New link goes to {database} for year {row.date}')
                new_producer_id = bd.get_node(
                        **{
                            "database": database, 
                            "name": previous_producer_node["name"],
                            "product": previous_producer_node["reference product"], 
                            "location": previous_producer_node["location"],  #TODO: should we also match on unit?
                        }
                    ).id   # Get new producer id by looking for the same activity in the new database
                # print(f'Previous producer: {previous_producer_node.key}, id = {previous_producer_id}')
                # print(f'Previous consumer: {bd.get_node(id=row.consumer).key}, id = {row.consumer}')
                # print(f'New producer id = {new_producer_id}')
                # print(f'New consumer id = {new_consumer_id}')
                # print()
                datapackage.add_persistent_vector(
                        matrix="technosphere_matrix",
                        name=uuid.uuid4().hex,
                        data_array=np.array([share], dtype=float),
                        indices_array=np.array(
                            [(new_producer_id, new_consumer_id)],
                            dtype=bwp.INDICES_DTYPE,
                        ),
                        flip_array=np.array([True], dtype=bool),
                )
    
    if not name:
        name = uuid.uuid4().hex # we dont use this variable? 
        # logger.info(f"Using random name {name}")

    if datapackage is None:
        datapackage = bwp.create_datapackage()

    new_nodes = set()
    consumer_timestamps = {}  # a dictionary to store the year of the consuming processes so that the inputs from previous times get linked right
    for row in timeline.iloc[::-1].itertuples():
        if row.consumer == -1:
            consumer_timestamps[row.consumer] = row.timestamp
        consumer_timestamps[row.producer] = row.timestamp  # the year of the producer will be the consumer year for this procuess until a it becomesa producer again
        # print(row.timestamp, row.producer, row.consumer, consumer_timestamps[row.consumer])
        add_row_to_datapackage(row,
                               datapackage,
                               database_date_dict,
                               demand_timing,
                               new_nodes,
                               consumer_timestamps,)
    
    # Adding ones on diagonal for new nodes
    datapackage.add_persistent_vector(
        matrix="technosphere_matrix",
        name=uuid.uuid4().hex,
        data_array=np.ones(len(new_nodes)),
        indices_array=np.array([(i, i) for i in new_nodes], dtype=bwp.INDICES_DTYPE),
    )

    return datapackage

def add_column_interpolation_weights_to_timeline(tl_df: pd.DataFrame, database_date_dict: dict, interpolation_type: str ="linear") -> pd.DataFrame:
    """
    Add a column to a timeline with the weights for an interpolation between the two nearest dates, from the list of dates from the available databases.

    :param tl_df: Timeline as a dataframe.
    :param database_date_dict: Mapping dictionary between the time of representativeness (key) and name of database (value)
    :param interpolation_type: Type of interpolation between the nearest lower and higher dates. For now, 
    only "linear" is available.
    
    :return: Timeline as a dataframe with a column 'interpolation_weights' (object:dictionnary) added.
    -------------------
    Example:
    >>> dates_list = [
        datetime.strptime("2020", "%Y"),
        datetime.strptime("2022", "%Y"),
        datetime.strptime("2025", "%Y"),
    ]
    >>> add_column_interpolation_weights_on_timeline(tl_df, dates_list, interpolation_type="linear")
    
    
    """
    
    
    
    def find_closest_date(target: datetime, dates: KeysView[datetime]) -> dict:
        """
        Find the closest date to the target in the dates list.
    
        :param target: Target datetime.datetime object.
        :param dates: List of datetime.datetime objects.
        :return: Dictionary with the key as the closest datetime.datetime object from the list and a value of 1.
    
        ---------------------
        # Example usage
        target = datetime.strptime("2023-01-15", "%Y-%m-%d")
        dates_list = [
            datetime.strptime("2020", "%Y"),
            datetime.strptime("2022", "%Y"),
            datetime.strptime("2025", "%Y"),
        ]
    
        print(closest_date(target, dates_list))
        """
    
        # If the list is empty, return None
        if not dates:
            return None
    
        # Sort the dates
        dates = sorted(dates)
        # Use min function with a key based on the absolute difference between the target and each date
        closest = min(dates, key=lambda date: abs(target - date))
    
        return {closest: 1}
    
    def get_weights_for_interpolation_between_nearest_years(reference_date: datetime, dates_list: KeysView[datetime], interpolation_type: str = "linear") -> dict:
        """
        Find the nearest dates (before and after) a given date in a list of dates and calculate the interpolation weights.
    
        :param reference_date: Target date.
        :param dates_list: KeysView[datetime], which is a list of temporal coverage of the available databases,.
        :param interpolation_type: Type of interpolation between the nearest lower and higher dates. For now, 
        only "linear" is available.
        
        :return: Dictionary with temporal coverage of the available databases to use as keys and the weights for interpolation as values.
        -------------------
        Example:
        >>> dates_list = [
            datetime.strptime("2020", "%Y"),
            datetime.strptime("2022", "%Y"),
            datetime.strptime("2025", "%Y"),
        ]
        >>> date_test = datetime(2021,10,11)
        >>> add_column_interpolation_weights_on_timeline(date_test, dates_list, interpolation_type="linear")
        """
        dates_list = sorted(dates_list)
        
        diff_dates_list = [reference_date - x for x in dates_list]
        if timedelta(0) in diff_dates_list:
            exact_match = dates_list[diff_dates_list.index(timedelta(0))]
            return {exact_match: 1}
        

        closest_lower = None
        closest_higher = None

        for date in dates_list:
            if date < reference_date:
                if closest_lower is None or reference_date - date < reference_date - closest_lower:
                    closest_lower = date
            elif date > reference_date:
                if closest_higher is None or date - reference_date < closest_higher - reference_date:
                    closest_higher = date
        
        if closest_lower is None:
            print(f"Warning: Reference date {reference_date} is lower than all provided dates. Data will be taken from closest higher year.")
            return {closest_higher: 1}
        
        if closest_higher is None:
            print(f"Warning: Reference date {reference_date} is higher than all provided dates. Data will be taken from the closest lower year.")
            return {closest_lower: 1}
  
        if closest_lower == closest_higher:
            warnings.warn("Date outside the range of dates covered by the databases.", category=Warning)
            return {closest_lower: 1}
            
        if interpolation_type == "linear":
            weight = int((reference_date - closest_lower).total_seconds())/int((closest_higher - closest_lower).total_seconds())
        else:
            raise ValueError(f"Sorry, but {interpolation_type} interpolation is not available yet.")
        return {closest_lower: 1-weight, closest_higher: weight}
    
    dates_list= database_date_dict.keys()
    if "date" not in list(tl_df.columns):
        raise ValueError("The timeline does not contain dates.")
        
    if interpolation_type == "nearest":
        tl_df['interpolation_weights'] = tl_df['date'].apply(lambda x: find_closest_date(x, dates_list))
        # change key of interplation weights dictionaries to database name instead of year
        tl_df['interpolation_weights'] = tl_df['interpolation_weights'].apply(lambda d: {database_date_dict[x]: v for x, v in d.items()})  
        return tl_df
    
    if interpolation_type == "linear":
        tl_df['interpolation_weights'] = tl_df['date'].apply(lambda x: get_weights_for_interpolation_between_nearest_years(x, dates_list, interpolation_type))
        # change key of interplation weights dictionaries to database name instead of year
        tl_df['interpolation_weights'] = tl_df['interpolation_weights'].apply(lambda d: {database_date_dict[x]: v for x, v in d.items()})
        
    else:
        raise ValueError(f"Sorry, but {interpolation_type} interpolation is not available yet.")
        
    return tl_df


def unpack(dct):
    for obj in dct:
        if hasattr(obj, "key"):
            yield obj.key
        else:
            yield obj

def prepare_medusa_lca_inputs(
    demand=None,
    method=None,
    demand_timing_dict=None,
    weighting=None,
    normalization=None,
    demands=None,
    remapping=True,
    demand_database_last=True,
):
    """
    Prepare LCA input arguments in Brightway 2.5 style. 
    ORIGINALLY FROM bw2data.compat.py

    Changes include:
    - always load all databases in demand_database_names
    - indexed_demand has the id of the new consumer_id of the "exploded" demand (TODO: think about more elegant way)
    
    """
    if not projects.dataset.data.get("25"):
        raise Brightway2Project(
            "Please use `projects.migrate_project_25` before calculating using Brightway 2.5"
        )

    databases.clean()
    data_objs = []
    remapping_dicts = None

    # if demands:
    #     demand_database_names = [
    #         db_label for dct in demands for db_label, _ in unpack(dct)
    #     ]
    # elif demand:
    #     demand_database_names = [db_label for db_label, _ in unpack(demand)]
    # else:
    #     demand_database_names = []

    demand_database_names = [db_label for db_label in databases] # Always load all databases

    

    if demand_database_names:
        database_names = set.union(
            *[
                Database(db_label).find_graph_dependents()
                for db_label in demand_database_names
            ]
        )

        if demand_database_last:
            database_names = [
                x for x in database_names if x not in demand_database_names
            ] + demand_database_names

        data_objs.extend([Database(obj).datapackage() for obj in database_names])

        if remapping:
            # This is technically wrong - we could have more complicated queries
            # to determine what is truly a product, activity, etc.
            # However, for the default database schema, we know that each node
            # has a unique ID, so this won't produce incorrect responses,
            # just too many values. As the dictionary only exists once, this is
            # not really a problem.
            reversed_mapping = {
                i: (d, c)
                for d, c, i in AD.select(AD.database, AD.code, AD.id)
                .where(AD.database << database_names)
                .tuples()
            }
            remapping_dicts = {
                "activity": reversed_mapping,
                "product": reversed_mapping,
                "biosphere": reversed_mapping,
            }

    if method:
        assert method in methods
        data_objs.append(Method(method).datapackage())
    if weighting:
        assert weighting in weightings
        data_objs.append(Weighting(weighting).datapackage())
    if normalization:
        assert normalization in normalizations
        data_objs.append(Normalization(normalization).datapackage())

    if demands:
        indexed_demand = [{get_id(k)*1000000+demand_timing_dict[get_id(k)]: v for k, v in dct.items()} for dct in demands] #why?
    elif demand:
        indexed_demand = {get_id(k)*1000000+demand_timing_dict[get_id(k)]: v for k, v in demand.items()}
    else:
        indexed_demand = None

    return indexed_demand, data_objs, remapping_dicts