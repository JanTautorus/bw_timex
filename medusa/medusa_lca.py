from datetime import datetime
from typing import Optional
import bw2data as bd
import warnings
import pandas as pd

# for prepare_medusa_lca_inputs
from bw2data import (
    Database,
    Method,
    Normalization,
    Weighting,
    databases,
    methods,
    normalizations,
    projects,
    weightings,
)
from bw2data.backends.schema import ActivityDataset as AD
from bw2data.backends.schema import get_id
from bw2data.errors import Brightway2Project

def create_demand_timing_dict(timeline: pd.DataFrame, demand: dict) -> dict:
    """
    Generate a dictionary mapping producer (key) to reference timing (currently YYYYMM) (value) for specific demands. 
    It searches the timeline for those rows that contain the functional units (demand-processes as producer and -1 as consumer) and returns the time of the demand.
    
    :param timeline: Timeline DataFrame, generated by `create_grouped_edge_dataframe`.
    :param demand: Demand dict
    
    :return: Dictionary mapping producer ids to reference timing (currently YYYYMM) for the specified demands.
    """
    demand_ids = [bd.get_activity(key).id for key in demand.keys()]
    demand_rows = timeline[timeline['producer'].isin(demand_ids) & (timeline['consumer'] == -1)]
    return {row.producer: row.hash_producer for row in demand_rows.itertuples()}  #old: extract_date_as_integer(row.date)

def extract_date_as_integer(dt_obj : datetime, time_res : Optional[str] ='year') -> int:
    """
    Converts a datetime object to an integer in the format YYYY 
    #FIXME: ideally we want to add YYYYMMDDHH to the ids, but this cretaes integers that are too long for 32-bit C long

    :param dt_obj: Datetime object.
    :time_res: time resolution to be returned: year=YYYY, month=YYYYMM, day=YYYYMMDD, hour=YYYYMMDDHH
    :return: INTEGER in the format YYYY.
    """
    time_res_dict = {'year':'%Y','month':'%Y%m','day':'%Y%m%d','hour':'%Y%m%d%M'}
    if time_res not in time_res_dict.keys():
        warnings.warn('time_res: {} is not a valid option. Please choose from: {} defaulting to "year"'.format(
                      time_res, time_res_dict.keys()), category=Warning)
    formatted_date = dt_obj.strftime('{}'.format(time_res_dict[time_res]))
    date_as_integer = int(formatted_date)

    return date_as_integer

def unpack(dct):
    for obj in dct:
        if hasattr(obj, "key"):
            yield obj.key
        else:
            yield obj

def prepare_medusa_lca_inputs(
    demand=None,
    method=None,
    demand_timing_dict=None,
    weighting=None,
    normalization=None,
    demands=None,
    remapping=True,
    demand_database_last=True,
):
    """
    Prepare LCA input arguments in Brightway 2.5 style. 
    ORIGINALLY FROM bw2data.compat.py

    Changes include:
    - always load all databases in demand_database_names
    - indexed_demand has the id of the new consumer_id of the "exploded" demand (TODO: think about more elegant way)
    
    """
    if not projects.dataset.data.get("25"):
        raise Brightway2Project(
            "Please use `projects.migrate_project_25` before calculating using Brightway 2.5"
        )

    databases.clean()
    data_objs = []
    remapping_dicts = None

    # if demands:
    #     demand_database_names = [
    #         db_label for dct in demands for db_label, _ in unpack(dct)
    #     ]
    # elif demand:
    #     demand_database_names = [db_label for db_label, _ in unpack(demand)]
    # else:
    #     demand_database_names = []

    demand_database_names = [db_label for db_label in databases] # Always load all databases

    

    if demand_database_names:
        database_names = set.union(
            *[
                Database(db_label).find_graph_dependents()
                for db_label in demand_database_names
            ]
        )

        if demand_database_last:
            database_names = [
                x for x in database_names if x not in demand_database_names
            ] + demand_database_names

        data_objs.extend([Database(obj).datapackage() for obj in database_names])

        if remapping:
            # This is technically wrong - we could have more complicated queries
            # to determine what is truly a product, activity, etc.
            # However, for the default database schema, we know that each node
            # has a unique ID, so this won't produce incorrect responses,
            # just too many values. As the dictionary only exists once, this is
            # not really a problem.
            reversed_mapping = {
                i: (d, c)
                for d, c, i in AD.select(AD.database, AD.code, AD.id)
                .where(AD.database << database_names)
                .tuples()
            }
            remapping_dicts = {
                "activity": reversed_mapping,
                "product": reversed_mapping,
                "biosphere": reversed_mapping,
            }

    if method:
        assert method in methods
        data_objs.append(Method(method).datapackage())
    if weighting:
        assert weighting in weightings
        data_objs.append(Weighting(weighting).datapackage())
    if normalization:
        assert normalization in normalizations
        data_objs.append(Normalization(normalization).datapackage())

    if demands:
        indexed_demand = [{get_id(k)*1000000+demand_timing_dict[get_id(k)]: v for k, v in dct.items()} for dct in demands] #why?
    elif demand:
        indexed_demand = {get_id(k)*1000000+demand_timing_dict[get_id(k)]: v for k, v in demand.items()}
    else:
        indexed_demand = None

    return indexed_demand, data_objs, remapping_dicts